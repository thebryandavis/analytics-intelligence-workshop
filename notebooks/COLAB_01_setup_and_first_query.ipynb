{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# Notebook 1: Setup & First Query\n",
    "\n",
    "## Welcome to Analytics Intelligence!\n",
    "\n",
    "**üëã New to Colab?** No problem! This is a cloud-based coding environment. You'll:\n",
    "- **Read** text cells like this one (markdown)\n",
    "- **Run** code cells by clicking the ‚ñ∂Ô∏è play button\n",
    "- **See** results appear below each code cell\n",
    "\n",
    "**üëÄ Just watching?** That's totally fine! You can follow along without running anything.\n",
    "\n",
    "---\n",
    "\n",
    "### What We're Building\n",
    "A system that watches your analytics data 24/7 and alerts you to:\n",
    "- üö® **Problems**: Tracking breaks, PII leaks, data quality issues\n",
    "- üéâ **Opportunities**: Traffic spikes, conversion improvements, new patterns\n",
    "- üìä **Insights**: Behavior shifts, emerging trends\n",
    "\n",
    "### Workshop Structure\n",
    "1. **This notebook (20 min)**: Connect to BigQuery, explore data, find first issue\n",
    "2. **Notebook 2 (20 min)**: Use AI to generate SQL checks\n",
    "3. **Notebook 3 (25 min)**: Build alerts and deploy\n",
    "\n",
    "### The Data\n",
    "We're using 7 days of sample news analytics (3.5M events, GA4-style) with **planted problems** and **opportunities** for you to discover.\n",
    "\n",
    "Let's go! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "---\n",
    "\n",
    "## Step 1: Install Dependencies\n",
    "\n",
    "**What this does:** Downloads the Python libraries we need (BigQuery connector, Pandas for data).\n",
    "\n",
    "**‚ñ∂Ô∏è Click the play button** to run this cell. Takes ~10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-cloud-bigquery pandas\n",
    "\n",
    "print(\"‚úì Packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "---\n",
    "\n",
    "## Step 2: Import Libraries\n",
    "\n",
    "**What this does:** \"Opens the toolbox\" - makes the libraries available to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports_code"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.colab import auth\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Display settings (makes tables easier to read)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auth"
   },
   "source": "---\n\n## Step 3: Authenticate with Google Cloud\n\n**What this does:** Logs you into Google Cloud so you can access BigQuery.\n\n**‚ö†Ô∏è You'll see a popup** asking to authorize. Click \"Allow\".\n\n**Note:** Bryan Davis (AP) has already set up a shared project for this workshop - you're getting read-only access to the sample data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auth_code"
   },
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud\n",
    "auth.authenticate_user()\n",
    "\n",
    "print(\"‚úì Authentication successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": "---\n\n## Step 4: Configure BigQuery Connection\n\n**What this does:** Points to the sample data project.\n\n**‚úÖ Already configured** - The project ID below points to Bryan's shared BigQuery dataset."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_code"
   },
   "outputs": [],
   "source": "# Configuration - UPDATE THIS VALUE\nPROJECT_ID = \"npa-workshop-2025\"  # ‚¨ÖÔ∏è Workshop project ID\nDATASET_ID = \"npa_workshop\"\nTABLE_ID = \"news_events\"\n\n# Build full table reference\nTABLE_REF = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n\nprint(f\"‚úì Configuration set!\")\nprint(f\"  Using table: {TABLE_REF}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "connect"
   },
   "source": [
    "---\n",
    "\n",
    "## Step 5: Connect to BigQuery\n",
    "\n",
    "**What this does:** Opens the connection to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "connect_code"
   },
   "outputs": [],
   "source": [
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Helper function to run queries easily\n",
    "def run_query(sql):\n",
    "    \"\"\"Execute SQL and return results as pandas DataFrame.\"\"\"\n",
    "    query_job = client.query(sql)\n",
    "    df = query_job.to_dataframe()\n",
    "    return df\n",
    "\n",
    "print(\"‚úì Connected to BigQuery!\")\n",
    "print(f\"  Project: {client.project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explore_intro"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 1: Explore the Data\n",
    "\n",
    "Before we can detect anomalies, we need to understand what's \"normal\" in our data.\n",
    "\n",
    "Think of this like getting to know a new friend - we're learning the baseline patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "schema"
   },
   "source": [
    "---\n",
    "\n",
    "## Table Schema\n",
    "\n",
    "**What this does:** Shows us what columns exist and what type of data they hold.\n",
    "\n",
    "This is like opening a CSV and looking at the headers before analyzing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "schema_code"
   },
   "outputs": [],
   "source": [
    "# Get table metadata\n",
    "table = client.get_table(TABLE_REF)\n",
    "\n",
    "print(f\"Table: {table.table_id}\")\n",
    "print(f\"Total Rows: {table.num_rows:,}\")\n",
    "print(f\"Size: {table.num_bytes / 1024 / 1024:.1f} MB\")\n",
    "print(f\"\\nSchema ({len(table.schema)} columns):\")\n",
    "print(\"-\" * 50)\n",
    "for field in table.schema:\n",
    "    print(f\"  {field.name:25} {field.field_type:10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sample"
   },
   "source": [
    "---\n",
    "\n",
    "## Sample Rows\n",
    "\n",
    "**What this does:** Shows us a few actual events to understand the data structure.\n",
    "\n",
    "**üìä Look for:**\n",
    "- `event_name`: What happened (page_view, scroll_depth, etc.)\n",
    "- `platform`: Where it happened (web, ios, android)\n",
    "- `consent_state`: GDPR flag (important!)\n",
    "- `engagement_time_msec`: How long they spent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sample_code"
   },
   "outputs": [],
   "source": [
    "# Get 5 sample rows\n",
    "sql = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{TABLE_REF}`\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "df_sample = run_query(sql)\n",
    "df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "date_range"
   },
   "source": [
    "---\n",
    "\n",
    "## Date Range\n",
    "\n",
    "**What this does:** Shows us what time period the data covers.\n",
    "\n",
    "**üéØ Expected:** 7 days, ~3.5M events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "date_range_code"
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  MIN(event_date) as first_date,\n",
    "  MAX(event_date) as last_date,\n",
    "  COUNT(DISTINCT event_date) as total_days,\n",
    "  COUNT(*) as total_events,\n",
    "  COUNT(DISTINCT user_pseudo_id) as unique_users\n",
    "FROM `{TABLE_REF}`\n",
    "\"\"\"\n",
    "\n",
    "df_range = run_query(sql)\n",
    "\n",
    "print(\"\\nüìÖ Data Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Date range: {df_range['first_date'][0]} ‚Üí {df_range['last_date'][0]}\")\n",
    "print(f\"  Days: {df_range['total_days'][0]}\")\n",
    "print(f\"  Total events: {df_range['total_events'][0]:,}\")\n",
    "print(f\"  Unique users: {df_range['unique_users'][0]:,}\")\n",
    "print(f\"  Avg events/day: {df_range['total_events'][0] / df_range['total_days'][0]:,.0f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baselines_intro"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 2: Establish Baselines\n",
    "\n",
    "To detect anomalies, we need to know what \"normal\" looks like.\n",
    "\n",
    "Let's look at typical patterns..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "events"
   },
   "source": [
    "---\n",
    "\n",
    "## Event Distribution\n",
    "\n",
    "**What this does:** Shows us what types of events we're tracking.\n",
    "\n",
    "**üéØ This is our baseline:** When we look for problems later, we're looking for deviations from these percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "events_code"
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  event_name,\n",
    "  COUNT(*) as event_count,\n",
    "  ROUND(COUNT(*) / (SELECT COUNT(*) FROM `{TABLE_REF}`) * 100, 1) as pct\n",
    "FROM `{TABLE_REF}`\n",
    "GROUP BY event_name\n",
    "ORDER BY event_count DESC\n",
    "\"\"\"\n",
    "\n",
    "df_events = run_query(sql)\n",
    "\n",
    "print(\"\\nüìä Event Type Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "print(df_events.to_string(index=False))\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° Insight: Page views dominate (expected for news sites)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "platform"
   },
   "source": [
    "---\n",
    "\n",
    "## Platform & Device Distribution\n",
    "\n",
    "**What this does:** Shows us web vs mobile app traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "platform_code"
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  platform,\n",
    "  device_category,\n",
    "  COUNT(*) as event_count,\n",
    "  ROUND(COUNT(*) / (SELECT COUNT(*) FROM `{TABLE_REF}`) * 100, 1) as pct\n",
    "FROM `{TABLE_REF}`\n",
    "GROUP BY platform, device_category\n",
    "ORDER BY event_count DESC\n",
    "\"\"\"\n",
    "\n",
    "df_platform = run_query(sql)\n",
    "\n",
    "print(\"\\nüì± Platform & Device Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "print(df_platform.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daily"
   },
   "source": [
    "---\n",
    "\n",
    "## Daily Event Volume\n",
    "\n",
    "**What this does:** Shows us how consistent daily traffic is.\n",
    "\n",
    "**üéØ We're looking for:** Consistent volume day-to-day (sudden drops = problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daily_code"
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  event_date,\n",
    "  COUNT(*) as event_count,\n",
    "  COUNT(DISTINCT user_pseudo_id) as unique_users,\n",
    "  ROUND(AVG(engagement_time_msec) / 1000, 1) as avg_engagement_sec\n",
    "FROM `{TABLE_REF}`\n",
    "GROUP BY event_date\n",
    "ORDER BY event_date\n",
    "\"\"\"\n",
    "\n",
    "df_daily = run_query(sql)\n",
    "\n",
    "print(\"\\nüìà Daily Volume:\")\n",
    "print(\"=\" * 70)\n",
    "print(df_daily.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate stats\n",
    "avg_daily = df_daily['event_count'].mean()\n",
    "std_daily = df_daily['event_count'].std()\n",
    "\n",
    "print(f\"\\nüìä Daily Statistics:\")\n",
    "print(f\"  Average: {avg_daily:,.0f} events/day\")\n",
    "print(f\"  Std Dev: {std_daily:,.0f}\")\n",
    "print(f\"  Range: {df_daily['event_count'].min():,} to {df_daily['event_count'].max():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hourly"
   },
   "source": [
    "---\n",
    "\n",
    "## Hourly Patterns\n",
    "\n",
    "**What this does:** Shows us when people read news throughout the day.\n",
    "\n",
    "**üéØ Expected pattern:** Low overnight, peaks at morning/lunch/evening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hourly_code"
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  EXTRACT(HOUR FROM event_datetime) as hour,\n",
    "  COUNT(*) as event_count,\n",
    "  ROUND(AVG(engagement_time_msec) / 1000, 1) as avg_engagement_sec\n",
    "FROM `{TABLE_REF}`\n",
    "WHERE event_name = 'page_view'\n",
    "GROUP BY hour\n",
    "ORDER BY hour\n",
    "\"\"\"\n",
    "\n",
    "df_hourly = run_query(sql)\n",
    "\n",
    "print(\"\\n‚è∞ Hourly Traffic Patterns:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Text-based visualization\n",
    "for _, row in df_hourly.iterrows():\n",
    "    bar = '‚ñà' * int(row['event_count'] / 10000)\n",
    "    print(f\"{int(row['hour']):02d}:00 | {bar} {row['event_count']:>8,} events\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nüí° Insight: Clear morning/lunch/evening peaks (typical news consumption)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quality_intro"
   },
   "source": [
    "---\n",
    "\n",
    "# Part 3: First Investigation\n",
    "\n",
    "Now that we know what \"normal\" looks like, let's check data quality.\n",
    "\n",
    "**üîç Specifically:** Are we missing critical fields?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "missing"
   },
   "source": [
    "---\n",
    "\n",
    "## Missing Values Check\n",
    "\n",
    "**What this does:** Counts how many events are missing important fields.\n",
    "\n",
    "**‚ö†Ô∏è Pay attention to `consent_state`** - this is a GDPR requirement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "missing_code"
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) as total_events,\n",
    "  \n",
    "  -- Missing counts\n",
    "  COUNTIF(consent_state IS NULL) as missing_consent,\n",
    "  COUNTIF(referrer IS NULL) as missing_referrer,\n",
    "  COUNTIF(article_id IS NULL) as missing_article,\n",
    "  COUNTIF(section IS NULL) as missing_section,\n",
    "  \n",
    "  -- Percentages\n",
    "  ROUND(COUNTIF(consent_state IS NULL) / COUNT(*) * 100, 1) as pct_missing_consent,\n",
    "  ROUND(COUNTIF(referrer IS NULL) / COUNT(*) * 100, 1) as pct_missing_referrer,\n",
    "  ROUND(COUNTIF(article_id IS NULL) / COUNT(*) * 100, 1) as pct_missing_article\n",
    "FROM `{TABLE_REF}`\n",
    "\"\"\"\n",
    "\n",
    "df_quality = run_query(sql)\n",
    "\n",
    "print(\"\\nüîç Data Quality Check:\")\n",
    "print(\"=\" * 70)\n",
    "print(df_quality.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check for issues\n",
    "missing_consent_pct = df_quality['pct_missing_consent'][0]\n",
    "\n",
    "print(\"\\nüö® Issue Detection:\")\n",
    "if missing_consent_pct > 10:\n",
    "    print(f\"\\n  ‚ö†Ô∏è  WARNING: {missing_consent_pct}% of events missing consent_state!\")\n",
    "    print(\"     This is a potential GDPR compliance problem.\")\n",
    "    print(\"     Legal/compliance teams should investigate immediately.\")\n",
    "else:\n",
    "    print(\"\\n  ‚úì Consent tracking looks healthy\")\n",
    "\n",
    "print(\"\\nüí° Insight: You just found your first data quality issue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "referrers"
   },
   "source": [
    "---\n",
    "\n",
    "## Traffic Sources\n",
    "\n",
    "**What this does:** Shows us where traffic is coming from.\n",
    "\n",
    "**üéØ This becomes important later** when we look for new referral sources (opportunities!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "referrers_code"
   },
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  referrer,\n",
    "  COUNT(*) as event_count,\n",
    "  COUNT(DISTINCT user_pseudo_id) as unique_users,\n",
    "  ROUND(AVG(engagement_time_msec) / 1000, 1) as avg_engagement_sec\n",
    "FROM `{TABLE_REF}`\n",
    "WHERE referrer IS NOT NULL\n",
    "GROUP BY referrer\n",
    "ORDER BY event_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df_referrers = run_query(sql)\n",
    "\n",
    "print(\"\\nüåê Top Traffic Sources:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_referrers.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exercise"
   },
   "source": [
    "---\n",
    "\n",
    "# üéØ Optional Exercise\n",
    "\n",
    "**For those following along:** Try writing your own query!\n",
    "\n",
    "**Goal:** Find the top 10 articles by page views.\n",
    "\n",
    "**Include:**\n",
    "- Article ID\n",
    "- Section\n",
    "- Number of page views\n",
    "- Number of unique users\n",
    "\n",
    "**Hint:** Filter to `event_name = 'page_view'` and use `GROUP BY`\n",
    "\n",
    "**Don't worry if you can't figure it out** - the solution is in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exercise_code"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (optional!)\n",
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  article_id,\n",
    "  section,\n",
    "  COUNT(*) as page_views,\n",
    "  COUNT(DISTINCT user_pseudo_id) as unique_users\n",
    "FROM `{TABLE_REF}`\n",
    "WHERE event_name = 'page_view'\n",
    "  AND article_id IS NOT NULL\n",
    "GROUP BY article_id, section\n",
    "ORDER BY page_views DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "df_top_articles = run_query(sql)\n",
    "\n",
    "print(\"\\nüèÜ Top 10 Articles:\")\n",
    "print(\"=\" * 80)\n",
    "print(df_top_articles.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "complete"
   },
   "source": "---\n\n# üéâ Notebook 1 Complete!\n\n## What You Accomplished\n\n‚úÖ Connected to BigQuery (no installation needed!)\n‚úÖ Explored 3.5M events across 7 days\n‚úÖ Established baselines for \"normal\" patterns\n‚úÖ Ran investigative SQL queries\n‚úÖ **Found your first issue**: 15% missing consent_state\n\n---\n\n## What You Discovered\n\nüìä **Data Profile:**\n- ~500K events/day over 7 days\n- 60% page views, 20% scroll events, 10% clicks\n- 50% web, 30% iOS, 20% Android\n- Clear hourly patterns (morning/lunch/evening peaks)\n\nüö® **Potential Problem:**\n- High percentage of missing `consent_state` values\n- This could be a GDPR compliance issue\n- Would you have caught this without automated checks?\n\n---\n\n## Next Steps\n\nIn **Notebook 2**, you'll:\n- ‚ú® Use **AI to automatically generate SQL** (no SQL expertise needed!)\n- üîç Detect **specific problems** (tracking breaks, PII leaks, duplicates)\n- üéâ Find **opportunities** (traffic spikes, new referrers)\n- üß† Learn **prompt engineering** for SQL generation\n\n**Ready to level up?** Open `COLAB_02_ai_generated_sql_checks.ipynb`\n\n---\n\n### Questions?\n\nAsk Bryan or reach out: **brdavis@ap.org**\n\n---\n\n**Presented by:**\nBryan Davis  \nDirector of Product, Data & Analytics  \nThe Associated Press\n\n---\n\n**üåü Pro Tip:** Bookmark this notebook - it's a great template for exploring any analytics dataset!"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Analytics Intelligence - Notebook 1: Setup & First Query",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}