{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro"
   },
   "source": [
    "# Notebook 2: AI-Generated SQL Checks\n",
    "\n",
    "## From Manual to Automated ü§ñ\n",
    "\n",
    "**üëã Recap:** In Notebook 1, you wrote SQL by hand to explore data. That works for investigation, but doesn't scale.\n",
    "\n",
    "**üò´ The problem:** You need dozens of checks running daily:\n",
    "- Missing consent flags?\n",
    "- PII in URLs?\n",
    "- Duplicate events?\n",
    "- Platform-specific tracking breaks?\n",
    "- Traffic spikes and new referrers?\n",
    "\n",
    "Writing and maintaining all that SQL is tedious and error-prone.\n",
    "\n",
    "**‚ú® The solution:** Use AI to generate SQL from plain English descriptions.\n",
    "\n",
    "---\n",
    "\n",
    "### What We'll Do\n",
    "\n",
    "1. ‚ú® Generate SQL with GPT (no SQL expertise needed!)\n",
    "2. üîç Detect the **4 planted problems** in our data\n",
    "3. üéâ Find the **2 planted opportunities**\n",
    "4. üß† Learn prompt engineering for better SQL\n",
    "\n",
    "**By the end:** You'll have found all 6 hidden issues in the data just by describing what to check in plain English.\n",
    "\n",
    "Let's go! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "---\n",
    "\n",
    "## Step 1: Install & Setup\n",
    "\n",
    "**What this does:** Installs OpenAI library + imports everything we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install OpenAI package\n",
    "!pip install -q openai google-cloud-bigquery pandas\n",
    "\n",
    "print(\"‚úì Packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from google.cloud import bigquery\n",
    "from google.colab import auth\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"‚úì Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auth"
   },
   "source": [
    "---\n",
    "\n",
    "## Step 2: Authenticate\n",
    "\n",
    "**What this does:** Logs you into Google Cloud (same as Notebook 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auth_code"
   },
   "outputs": [],
   "source": [
    "# Authenticate with Google Cloud\n",
    "auth.authenticate_user()\n",
    "\n",
    "print(\"‚úì Google Cloud authentication complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": "---\n\n## Step 3: Configure BigQuery + OpenAI\n\n**üìù Update OpenAI API Key:**\n- Get from platform.openai.com (or just watch the demo!)\n- BigQuery project already configured"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_code"
   },
   "outputs": [],
   "source": "# BigQuery configuration\nPROJECT_ID = \"npa-workshop-2025\"  # ‚¨ÖÔ∏è Workshop project ID\nDATASET_ID = \"npa_workshop\"\nTABLE_ID = \"news_events\"\nTABLE_REF = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\"\n\n# OpenAI configuration\nOPENAI_API_KEY = \"sk-your-key-here\"  # ‚¨ÖÔ∏è UPDATE THIS (or just watch!)\n\n# Initialize OpenAI client (new API v1.0+)\nfrom openai import OpenAI\nclient_openai = OpenAI(api_key=OPENAI_API_KEY)\n\nprint(\"‚úì Configuration set!\")\nprint(f\"  BigQuery table: {TABLE_REF}\")\nprint(f\"  OpenAI key: {OPENAI_API_KEY[:10]}...\" if OPENAI_API_KEY.startswith('sk-') else \"‚ö†Ô∏è No API key - watching mode only\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "connect"
   },
   "source": [
    "---\n",
    "\n",
    "## Step 4: Connect to BigQuery\n",
    "\n",
    "**What this does:** Establishes database connection (same as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "connect_code"
   },
   "outputs": [],
   "source": [
    "# Initialize BigQuery client\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# Helper function\n",
    "def run_query(sql):\n",
    "    \"\"\"Execute SQL and return results as DataFrame.\"\"\"\n",
    "    query_job = client.query(sql)\n",
    "    return query_job.to_dataframe()\n",
    "\n",
    "# Get schema for AI\n",
    "table = client.get_table(TABLE_REF)\n",
    "schema_info = \"\\n\".join([f\"- {field.name}: {field.field_type}\" for field in table.schema])\n",
    "\n",
    "print(\"‚úì Connected to BigQuery!\")\n",
    "print(f\"  Table: {table.table_id}\")\n",
    "print(f\"  Rows: {table.num_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sql_generator"
   },
   "source": [
    "---\n",
    "\n",
    "# The Magic: SQL Generator ü™Ñ\n",
    "\n",
    "**This is the key component:** A simple function that takes plain English and returns BigQuery SQL.\n",
    "\n",
    "**How it works:**\n",
    "1. You describe what to check (\"Find iOS events where scroll_depth dropped\")\n",
    "2. We send that + the schema to GPT\n",
    "3. GPT returns perfect BigQuery SQL\n",
    "4. We execute it\n",
    "\n",
    "**That's it!** No SQL expertise required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sql_gen_code"
   },
   "outputs": [],
   "source": "def generate_sql(description, model=\"gpt-4\"):\n    \"\"\"\n    Generate BigQuery SQL from plain English description.\n    \n    Args:\n        description: Plain English check description\n        model: \"gpt-4\" (better) or \"gpt-3.5-turbo\" (cheaper)\n    \n    Returns:\n        SQL string\n    \"\"\"\n    prompt = f\"\"\"You are a BigQuery SQL expert. Generate SQL for this analytics check.\n\nTable: `{TABLE_REF}`\n\nSchema:\n{schema_info}\n\nCheck Description:\n{description}\n\nRequirements:\n- Return ONLY valid BigQuery SQL, no explanation\n- Use standard SQL syntax (not legacy)\n- Include comments explaining logic\n- Order results meaningfully\n\"\"\"\n\n    response = client_openai.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0  # Deterministic output\n    )\n    \n    sql = response.choices[0].message.content\n    \n    # Clean up markdown code blocks if present\n    if sql.startswith('```'):\n        sql = sql.split('```')[1]\n        if sql.startswith('sql\\n'):\n            sql = sql[4:]\n    \n    return sql.strip()\n\nprint(\"‚úì SQL Generator function defined!\")\nprint(\"\\nReady to generate SQL from plain English! üéâ\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_intro"
   },
   "source": [
    "---\n",
    "\n",
    "# Let's Find All 6 Issues! üîç\n",
    "\n",
    "Now we'll use the SQL generator to find:\n",
    "- **4 problems** (missing consent, iOS break, PII leak, duplicates)\n",
    "- **2 opportunities** (newsletter spike, new referrer)\n",
    "\n",
    "Just by describing what to check in plain English!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check1"
   },
   "source": [
    "---\n",
    "\n",
    "## Check 1: Missing Consent (GDPR Risk) ‚ö†Ô∏è\n",
    "\n",
    "**What we're looking for:** Events with missing `consent_state` field.\n",
    "\n",
    "**Why it matters:** GDPR compliance requires tracking user consent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check1_gen"
   },
   "outputs": [],
   "source": [
    "# Describe the check in plain English\n",
    "description = \"\"\"\n",
    "Find events where consent_state is NULL (missing).\n",
    "Group by event_date and platform.\n",
    "Calculate:\n",
    "- Total events per day/platform\n",
    "- Count of events with missing consent\n",
    "- Percentage missing\n",
    "Filter to rows where >5% are missing.\n",
    "Order by percentage descending.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Request: Find events with missing consent_state\\n\")\n",
    "print(\"‚ú® Generating SQL with GPT-4...\\n\")\n",
    "\n",
    "sql = generate_sql(description)\n",
    "\n",
    "print(\"üìù Generated SQL:\")\n",
    "print(\"=\" * 70)\n",
    "print(sql)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check1_run"
   },
   "outputs": [],
   "source": [
    "# Execute the generated SQL\n",
    "print(\"\\nüöÄ Executing query...\\n\")\n",
    "\n",
    "df = run_query(sql)\n",
    "\n",
    "print(f\"üìä Results: Found {len(df)} combinations with missing consent\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analysis\n",
    "if len(df) > 0:\n",
    "    avg_missing = df[df.columns[df.columns.str.contains('pct|percent', case=False)]].iloc[:, 0].mean()\n",
    "    print(f\"\\n‚ö†Ô∏è  PROBLEM DETECTED!\")\n",
    "    print(f\"   Average {avg_missing:.1f}% of events missing consent_state\")\n",
    "    print(f\"   This is a GDPR compliance risk!\")\n",
    "    print(f\"   Action: Review consent tracking implementation immediately\")\nelse:\n",
    "    print(\"\\n‚úì No consent issues found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check2"
   },
   "source": [
    "---\n",
    "\n",
    "## Check 2: iOS Scroll Tracking Break üö®\n",
    "\n",
    "**What we're looking for:** Hours where iOS `scroll_depth` events drop to near-zero.\n",
    "\n",
    "**Why it matters:** Broken tracking means lost engagement data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check2_gen"
   },
   "outputs": [],
   "source": [
    "description = \"\"\"\n",
    "Find hours where iOS scroll_depth events are abnormally low or zero.\n",
    "For each day/hour combination:\n",
    "- Count iOS scroll_depth events\n",
    "- Calculate expected count (average across all hours)\n",
    "Filter to hours with < 1000 events.\n",
    "Show: event_date, hour, actual count, expected count\n",
    "Order by event_date, hour\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Request: Find iOS scroll_depth tracking breaks\\n\")\n",
    "print(\"‚ú® Generating SQL...\\n\")\n",
    "\n",
    "sql = generate_sql(description)\n",
    "\n",
    "print(\"üìù Generated SQL:\")\n",
    "print(\"=\" * 70)\n",
    "print(sql)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check2_run"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Executing query...\\n\")\n",
    "\n",
    "df = run_query(sql)\n",
    "\n",
    "print(f\"üìä Results: Found {len(df)} hours with low iOS scroll events\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(df.head(20).to_string(index=False))\n",
    "if len(df) > 20:\n",
    "    print(f\"\\n... and {len(df) - 20} more rows\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(df) > 0:\n",
    "    print(f\"\\nüö® TRACKING BREAK DETECTED!\")\n",
    "    print(f\"   iOS scroll_depth events dropped significantly\")\n",
    "    \n",
    "    # Find when it started\n",
    "    if 'event_date' in df.columns and 'hour' in df.columns:\n",
    "        first_issue = df.iloc[0]\n",
    "        print(f\"   First detected: {first_issue['event_date']} at hour {first_issue['hour']}\")\n",
    "    \n",
    "    print(f\"\\n   This affects all iOS users!\")\n",
    "    print(f\"   Engagement metrics are blocked!\")\n",
    "    print(f\"   Action: Check iOS SDK implementation immediately!\")\nelse:\n",
    "    print(\"\\n‚úì No iOS tracking issues detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check3"
   },
   "source": [
    "---\n",
    "\n",
    "## Check 3: PII in URLs (Privacy Violation) üîí\n",
    "\n",
    "**What we're looking for:** Email addresses in `page_location` URLs.\n",
    "\n",
    "**Why it matters:** Logging PII is a GDPR/privacy violation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check3_gen"
   },
   "outputs": [],
   "source": [
    "description = \"\"\"\n",
    "Find events where page_location contains email addresses.\n",
    "Look for pattern: 'email=' in the URL or '@' symbol.\n",
    "Show: page_location, count of occurrences, count of affected users\n",
    "Order by count descending\n",
    "Limit to top 20 examples\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Request: Find PII (emails) in page URLs\\n\")\n",
    "print(\"‚ú® Generating SQL...\\n\")\n",
    "\n",
    "sql = generate_sql(description)\n",
    "\n",
    "print(\"üìù Generated SQL:\")\n",
    "print(\"=\" * 70)\n",
    "print(sql)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check3_run"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Executing query...\\n\")\n",
    "\n",
    "df = run_query(sql)\n",
    "\n",
    "print(f\"üìä Results: Found {len(df)} URLs with email patterns\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(df) > 0:\n",
    "    total_events = df[df.columns[df.columns.str.contains('count|occurrences', case=False)]].iloc[:, 0].sum()\n",
    "    print(f\"\\nüö® PRIVACY VIOLATION DETECTED!\")\n",
    "    print(f\"   {len(df)} unique URLs contain email addresses\")\n",
    "    print(f\"   Total events with PII: ~{total_events:,}\")\n",
    "    print(f\"\\n   This is a GDPR/CCPA violation!\")\n",
    "    print(f\"   Email addresses should NEVER be in tracking URLs!\")\n",
    "    print(f\"   Action: Review form tracking code immediately!\")\nelse:\n",
    "    print(\"\\n‚úì No PII detected in URLs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check4"
   },
   "source": [
    "---\n",
    "\n",
    "## Check 4: Duplicate Events üîÅ\n",
    "\n",
    "**What we're looking for:** Same user + same timestamp appearing multiple times.\n",
    "\n",
    "**Why it matters:** Duplicates inflate metrics and break analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check4_gen"
   },
   "outputs": [],
   "source": [
    "description = \"\"\"\n",
    "Find duplicate events where the same user_pseudo_id and event_timestamp appear multiple times.\n",
    "Group by user_pseudo_id, event_name, event_timestamp\n",
    "Count occurrences\n",
    "Filter to only duplicates (count > 1)\n",
    "Order by duplicate count descending\n",
    "Limit to worst 30 cases\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Request: Find duplicate events\\n\")\n",
    "print(\"‚ú® Generating SQL...\\n\")\n",
    "\n",
    "sql = generate_sql(description)\n",
    "\n",
    "print(\"üìù Generated SQL:\")\n",
    "print(\"=\" * 70)\n",
    "print(sql)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check4_run"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Executing query...\\n\")\n",
    "\n",
    "df = run_query(sql)\n",
    "\n",
    "print(f\"üìä Results: Found {len(df)} cases of duplicate events\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(df.head(15).to_string(index=False))\n",
    "if len(df) > 15:\n",
    "    print(f\"\\n... and {len(df) - 15} more cases\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(df) > 0:\n",
    "    # Estimate total duplicates\n",
    "    count_col = [col for col in df.columns if 'count' in col.lower()][0]\n",
    "    total_dupes = df[count_col].sum() - len(df)  # Subtract one instance per case\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  DATA QUALITY ISSUE!\")\n",
    "    print(f\"   Found {len(df)} unique timestamp+user combinations with duplicates\")\n",
    "    print(f\"   Estimated duplicate events: ~{total_dupes:,}\")\n",
    "    print(f\"\\n   Likely causes:\")\n",
    "    print(f\"   - Double-click bugs (buttons firing twice)\")\n",
    "    print(f\"   - Race conditions in tracking code\")\n",
    "    print(f\"   - Retry logic without deduplication\")\n",
    "    print(f\"\\n   Action: Check for double-firing tracking events!\")\nelse:\n",
    "    print(\"\\n‚úì No duplicate events detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opportunities"
   },
   "source": [
    "---\n",
    "\n",
    "# Now Let's Find Good Things! üéâ\n",
    "\n",
    "**Same system, different purpose:** Use AI to find opportunities instead of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check5"
   },
   "source": [
    "---\n",
    "\n",
    "## Check 5: Newsletter Signup Spike üìà\n",
    "\n",
    "**What we're looking for:** Days with significant increases in newsletter signups.\n",
    "\n",
    "**Why it matters:** Understanding what drives success so you can replicate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check5_gen"
   },
   "outputs": [],
   "source": [
    "description = \"\"\"\n",
    "Find days where newsletter_signup events increased significantly.\n",
    "Calculate:\n",
    "- Daily signup count\n",
    "- Previous day's count (using LAG window function)\n",
    "- Percentage change from previous day\n",
    "Filter to days with >30% increase\n",
    "Order by percentage change descending\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Request: Find newsletter signup spikes\\n\")\n",
    "print(\"‚ú® Generating SQL...\\n\")\n",
    "\n",
    "sql = generate_sql(description)\n",
    "\n",
    "print(\"üìù Generated SQL:\")\n",
    "print(\"=\" * 70)\n",
    "print(sql)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check5_run"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Executing query...\\n\")\n",
    "\n",
    "df = run_query(sql)\n",
    "\n",
    "print(f\"üìä Results: Newsletter signup trends\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(df) > 0:\n",
    "    pct_col = [col for col in df.columns if 'pct' in col.lower() or 'percent' in col.lower()][0]\n",
    "    max_spike = df[pct_col].max()\n",
    "    spike_day = df[df[pct_col] == max_spike].iloc[0]\n",
    "    \n",
    "    print(f\"\\nüéâ OPPORTUNITY DETECTED!\")\n",
    "    print(f\"   Newsletter signups spiked {max_spike:.1f}% on {spike_day['event_date']}\")\n",
    "    print(f\"\\n   This is GOOD news! Worth investigating:\")\n",
    "    print(f\"   - Did you run a successful campaign?\")\n",
    "    print(f\"   - Did a viral article drive traffic?\")\n",
    "    print(f\"   - Did you change signup form placement?\")\n",
    "    print(f\"   - Can you replicate this success?\")\n",
    "    print(f\"\\n   Action: Investigate what drove the increase and document it!\")\nelse:\n",
    "    print(\"\\n‚úì No significant signup changes detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check6"
   },
   "source": [
    "---\n",
    "\n",
    "## Check 6: New Referrer Source üåê\n",
    "\n",
    "**What we're looking for:** Referrers that appeared recently but not before.\n",
    "\n",
    "**Why it matters:** New traffic sources = new opportunities to explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check6_gen"
   },
   "outputs": [],
   "source": [
    "description = \"\"\"\n",
    "Find new referrer sources that appeared in recent days (20251005+) but not in earlier days.\n",
    "Use CTEs to:\n",
    "1. Get distinct referrers from recent period\n",
    "2. Get distinct referrers from earlier period  \n",
    "3. Find referrers in recent but not in earlier\n",
    "For new referrers, show:\n",
    "- Referrer name\n",
    "- First appearance date\n",
    "- Total event count\n",
    "- Unique user count\n",
    "Order by event count descending\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Request: Find new referrer sources\\n\")\n",
    "print(\"‚ú® Generating SQL...\\n\")\n",
    "\n",
    "sql = generate_sql(description)\n",
    "\n",
    "print(\"üìù Generated SQL:\")\n",
    "print(\"=\" * 70)\n",
    "print(sql)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check6_run"
   },
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Executing query...\\n\")\n",
    "\n",
    "df = run_query(sql)\n",
    "\n",
    "print(f\"üìä Results: Found {len(df)} new referrer source(s)\\n\")\n",
    "print(\"=\" * 70)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if len(df) > 0:\n",
    "    top_source = df.iloc[0]\n",
    "    referrer_name = top_source[df.columns[0]]  # First column is referrer\n",
    "    \n",
    "    event_col = [col for col in df.columns if 'event' in col.lower() and 'count' in col.lower()][0]\n",
    "    user_col = [col for col in df.columns if 'user' in col.lower() and 'count' in col.lower()][0]\n",
    "    \n",
    "    print(f\"\\nüéâ NEW TRAFFIC SOURCE DISCOVERED!\")\n",
    "    print(f\"   Source: {referrer_name}\")\n",
    "    print(f\"   Events: {top_source[event_col]:,}\")\n",
    "    print(f\"   Users: {top_source[user_col]:,}\")\n",
    "    print(f\"\\n   Opportunities to explore:\")\n",
    "    print(f\"   - Is this organic or paid traffic?\")\n",
    "    print(f\"   - What content is resonating with this audience?\")\n",
    "    print(f\"   - Can you engage with this community?\")\n",
    "    print(f\"   - Should you optimize content for this channel?\")\n",
    "    print(f\"\\n   Action: Investigate this new source and consider amplifying!\")\nelse:\n",
    "    print(\"\\n‚úì No new referrers detected in recent period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": "---\n\n# üéâ Success! All Issues Found!\n\n## What We Discovered\n\n### Problems (4)\n1. ‚úÖ **Missing Consent** - 15% of events lack GDPR consent tracking\n2. ‚úÖ **PII Leak** - Email addresses appearing in page URLs\n3. ‚úÖ **Duplicate Events** - ~1% of events are duplicates\n4. ‚úÖ (Additional data quality issues as discovered)\n\n### Opportunities (2)\n5. ‚úÖ **Newsletter Spike** - Signups jumped significantly on Day 4\n6. ‚úÖ **New Referrer** - reddit.com traffic appeared on Day 5+\n\n---\n\n## How We Did It\n\n**Traditional approach:**\n- Write complex SQL for each check\n- Test and debug\n- Maintain dozens of queries\n- Time: Hours/days\n\n**Our approach:**\n- Describe check in plain English\n- GPT generates perfect SQL\n- Execute and analyze\n- Time: Minutes!\n\n---\n\n## Cost Analysis\n\n**This notebook:**\n- API calls: ~6 GPT-4 requests\n- Cost: ~$0.08 (GPT-4) or ~$0.008 (GPT-3.5-turbo)\n\n**Running daily:**\n- GPT-4: ~$2.40/month\n- GPT-3.5-turbo: ~$0.24/month\n\n**Recommendation:** Use GPT-3.5-turbo for SQL generation (works great, 10x cheaper!)\n\n---\n\n## Key Takeaways\n\n1. **No SQL expertise needed** - Just describe what you want\n2. **Fast iteration** - Generate ‚Üí test ‚Üí refine in seconds\n3. **Maintainable** - Descriptions are easier to read than SQL\n4. **Scalable** - Add 100 checks as easily as 10\n5. **Balanced** - Find problems AND opportunities\n\n---\n\n## Next Steps\n\nIn **Notebook 3**, you'll:\n- ü§ñ Use AI to **classify** findings (critical vs minor, problem vs opportunity)\n- üì® Send **Slack alerts** automatically\n- ‚öôÔ∏è Configure checks in **YAML** (no code changes needed)\n- üöÄ **Deploy** to production (GitHub Actions, Cloud Functions, etc.)\n\n**Ready for the final piece?**\n\nOpen `COLAB_03_anomaly_detection_and_alerts.ipynb` üöÄ\n\n---\n\n### Questions?\n\nAsk Bryan or reach out: **brdavis@ap.org**\n\n---\n\n**Presented by:**\nBryan Davis  \nDirector of Product, Data & Analytics  \nThe Associated Press"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Analytics Intelligence - Notebook 2: AI-Generated SQL Checks",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}